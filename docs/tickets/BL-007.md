# BL-007 — Suggestions Service (provider interface + streaming)

Status: Planned  
Owner: TBA  
Labels: backend, suggestions, streaming, httpx, resilience  
Blocks: BL-008 (Suggestions WebSocket consumer), BL-015 (Suggestions UI)  
Depends on: BL-004 (models), BL-005 (DRF endpoints/base)

## Summary
Introduce a pluggable Suggestions Service that streams AI-generated text from an OpenAI-compatible HTTP API. The service exposes a provider interface, implements an `httpx` streaming client with retries and timeouts, and persists streaming events (`delta`, `usage`, `done`, `error`) to `SuggestionEvent`. It returns a session identifier and an async event stream suitable for a WebSocket consumer (BL-008) to relay to clients. Cancellation and graceful shutdown are supported.

## Goals
- Provider protocol for AI streaming (OpenAI-compatible baseline).
- `httpx` async streaming client with:
  - bounded timeouts (connect/read/write/total),
  - retry with exponential backoff + jitter on transient errors,
  - cancellation support.
- Service orchestrator that:
  - creates `Suggestion` records with `session_id`,
  - parses provider stream into normalized events,
  - persists `SuggestionEvent` rows,
  - yields events to callers in real-time,
  - updates `Suggestion.status` and summary payload on completion/error.
- Event schema aligned with `SuggestionEventType`: `delta`, `usage`, `done`, `error`.
- Clear interfaces for BL-008 consumer to subscribe/yield/cancel.

## Non-Goals
- WebSocket consumer and routing (covered by BL-008).
- Frontend UI and accept/reject flows (BL-015 and later).
- Provider-specific advanced features beyond basic chat/completions.

## Service Surface (internal API)
- `apps.suggestions.providers.base.AIProvider` (Protocol)
  - `async def stream(prompt: str, *, settings: ProviderSettings, context: ProviderContext, cancel: CancelToken) -> AsyncIterator[ProviderEvent]]`
- `apps.suggestions.providers.openai.OpenAIProvider` (OpenAI-compatible implementation)
- `apps.suggestions.services.stream.SuggestionsService`
  - `async def start(chapter_id: int, prompt: str, *, settings: SuggestionSettings, context: SuggestionContext) -> tuple[uuid.UUID, AsyncIterator[SuggestionEventPayload]]]`
  - `def cancel(session_id: uuid.UUID) -> None` (idempotent)
- Event payloads normalized to:
  - `{"type": "delta", "value": str}`
  - `{"type": "usage", "prompt_tokens": int, "completion_tokens": int, "total_tokens": int}`
  - `{"type": "done"}`
  - `{"type": "error", "message": str}`

## Schemas (shapes)
- `SuggestionSettings`: `model: str`, `temperature: float`, `max_tokens: int | None`, `stop: list[str] | None`, `timeout_s: int = 120`
- `SuggestionContext`: `{ system_prompt: str, personas: list[str], chapter_titles: list[str] }` (MVP minimal)
- `ProviderEvent` union: `Delta | Usage | Done | Error`
- Persisted events map to `SuggestionEvent(event_type, payload)`.

## Provider Contract (OpenAI-compatible)
- HTTP: `POST /v1/chat/completions` with `stream: true`
- Request (example):
  ```json
  {
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "system", "content": "<context/system prompt>"},
      {"role": "user", "content": "<prompt>"}
    ],
    "temperature": 0.7,
    "max_tokens": 512,
    "stream": true
  }
  ```
- Stream format:
  - Server-Sent Events-style lines: `data: { ... }`
  - End sentinel: `data: [DONE]`
  - Parse incremental deltas from `choices[].delta.content`
  - Usage (if emitted at end or via separate summary) mapped to `usage` event.

## Cancellation & Timeouts
- `CancelToken` propagated to provider; checked between chunks.
- `httpx` timeouts: `connect=10s`, `read=45s`, `write=45s`, `pool=None`, `timeout_total=settings.timeout_s`.
- On cancel: emit `{"type": "done"}` and close response stream gracefully.

## Error Handling
- Transient errors (HTTP 429/5xx, connection reset, read timeouts):
  - Retry up to 2 times with exponential backoff (base 0.5s, factor 2.0, jitter ±0.25s).
- Non-retryable (401/403/4xx except 429) → emit `error`, set `Suggestion.status="rejected"`.
- Always close stream and finalize `Suggestion.status` (`pending` → `pending` on normal done; `rejected` on error).
- Persist an `error` event payload with `code`, `message`, `retryable`.

## Implementation Plan
- Providers
  - `apps/suggestions/providers/base.py`
    - `AIProvider` Protocol; `ProviderSettings`, `ProviderContext` dataclasses/TypedDicts.
    - `ProviderEvent` types and helpers (`is_delta`, `is_usage`, etc.).
  - `apps/suggestions/providers/openai.py`
    - `OpenAIProvider` using `httpx.AsyncClient`.
    - SSE parser: iterate `aiter_lines()`, filter `data:` lines, decode JSON; handle `[DONE]`.
    - Map OpenAI chunks to normalized events.
    - Env/config: `OPENAI_API_BASE`, `OPENAI_API_KEY`, default model.
- Orchestrator
  - `apps/suggestions/services/stream.py`
    - `SuggestionsService.start(...)`:
      1. Create `Suggestion(session_id=uuid4(), chapter=..., status="pending")`.
      2. Build system prompt from `SuggestionContext`.
      3. `async for event in provider.stream(...):`
         - Persist `SuggestionEvent`.
         - Aggregate minimal `payload` summary (e.g., token counts).
         - `yield` normalized event to caller.
      4. On normal end: keep `status="pending"` (accept/reject later tickets).
      5. On error: set `status="rejected"`, persist final `error` event.
    - Maintain a registry `{session_id: CancelToken}` and `cancel(session_id)`.
- Configuration
  - Factory `get_provider()` based on env `SUGGESTIONS_PROVIDER=openai|mock`.
  - Defaults safe for local dev; mock provider for tests.

## Pseudo-code

```py
# providers/base.py
class AIProvider(Protocol):
    async def stream(self, *, prompt: str, settings: ProviderSettings, context: ProviderContext, cancel: CancelToken) -> AsyncIterator[ProviderEvent]: ...

# services/stream.py
class SuggestionsService:
    _sessions: dict[UUID, CancelToken] = {}

    @classmethod
    async def start(cls, chapter_id: int, prompt: str, *, settings: SuggestionSettings, context: SuggestionContext) -> tuple[UUID, AsyncIterator[SuggestionEventPayload]]:
        ch = Chapter.objects.get(pk=chapter_id)
        s = Suggestion.objects.create(chapter=ch, session_id=uuid4(), status=SuggestionStatus.PENDING)
        cancel = CancelToken()
        cls._sessions[s.session_id] = cancel
        provider = get_provider()

        async def gen():
            try:
                async for ev in provider.stream(prompt=prompt, settings=settings, context=context, cancel=cancel):
                    SuggestionEvent.objects.create(suggestion=s, event_type=map_type(ev), payload=ev)
                    yield normalize(ev)
                # done
                yield {"type": "done"}
            except ProviderError as e:
                SuggestionEvent.objects.create(suggestion=s, event_type=SuggestionEventType.ERROR, payload={"message": str(e), "retryable": e.retryable})
                s.status = SuggestionStatus.REJECTED
                s.save(update_fields=["status", "updated_at"])
                yield {"type": "error", "message": str(e)}
            finally:
                cls._sessions.pop(s.session_id, None)

        return s.session_id, gen()

    @classmethod
    def cancel(cls, session_id: UUID) -> None:
        if token := cls._sessions.get(session_id):
            token.cancel()
```

## Tests (pytest)
- Provider (unit, with `respx`):
  - Parses stream into `delta` chunks (including unicode).
  - Handles `[DONE]`, emits `done`.
  - Retries on 429/5xx and timeouts; gives up after 2 retries.
  - Honors cancellation mid-stream.
- Orchestrator (integration-ish):
  - Creates `Suggestion` and `SuggestionEvent` records in order.
  - Yields events as they are persisted.
  - Sets status to `rejected` on provider error; leaves as `pending` on success.
  - Large prompt/content unicode path.
- No network in tests; all provider calls mocked.

## OpenAPI & Types
- No schema changes in this ticket (internal service). Endpoint to start sessions will be defined in a later ticket (BL-008/BL-015).

## Acceptance Criteria
- Provider Protocol defined; OpenAI-compatible provider implemented with streaming.
- Service exposes `start(...)` returning `(session_id, async iterator)` and `cancel(session_id)`.
- Events normalized and persisted; ordering preserved.
- Retries implemented with bounded backoff; hardened timeouts.
- Cancellation supported and verified by tests.
- Logging present for start/stop, retries, and errors.
- All new tests pass locally (`pytest`), strict type checks are green.

## Request/Response Examples (normalized events)
```
{ "type": "delta", "value": "The breeze carried a distant" }
{ "type": "delta", "value": " whisper across the pier." }
{ "type": "usage", "prompt_tokens": 118, "completion_tokens": 92, "total_tokens": 210 }
{ "type": "done" }
```

## Risks & Mitigations
- Provider drift: normalize events; keep parsing tolerant to shape changes.
- Network flakiness: retries with jitter; sane timeouts; early cancel.
- Memory growth: stream/flush per chunk; avoid buffering entire output.
- Ordering guarantees: DB writes are sequential per session; single writer per stream.

## Estimation
- Effort: M (1–2 days including provider, service, and tests).

## Checklist
- [ ] Provider protocol and event types.
- [ ] OpenAI provider with streaming + retries.
- [ ] SuggestionsService with start/cancel and persistence.
- [ ] Tests (provider + service).
- [ ] Docs updated (this ticket), basic usage notes in `docs/00-draft.md` confirmed.
